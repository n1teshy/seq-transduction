{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbHeQnUwuNsU",
        "outputId": "6919ebef-7ad4-44b1-fe2c-ff1381af7e6e"
      },
      "outputs": [],
      "source": [
        "# Get source files\n",
        "!git clone https://github.com/n1teshy/sequence-transduction && rm sequence-transduction/main.ipynb && mv sequence-transduction/* . && rm -rf sequence-transduction\n",
        "# Get data and tokenizers\n",
        "!git clone https://github.com/n1teshy/cache && mv cache/de_en/data cache/de_en/tokenizers . && rm -rf cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from core.tokenizers.regex import get_tokenizer\n",
        "from core.datasets.text import TranslationDataset\n",
        "from core.models import Transformer\n",
        "from core.utils import get_param_count, kaiming_init\n",
        "from core.config import device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUudrzW9uMMB"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "train_de, train_en = \"data/de_train.txt\", \"data/en_train.txt\"\n",
        "val_de, val_en = \"data/de_val.txt\", \"data/en_val.txt\"\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "de_tokenizer = get_tokenizer(\"de.txt\", 512, \"tokenizers/de\", True)\n",
        "en_tokenizer = get_tokenizer(\"en.txt\", 512, \"tokenizers/en\", True)\n",
        "\n",
        "train_dataset = TranslationDataset(train_de, train_en, de_tokenizer, en_tokenizer)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate\n",
        ")\n",
        "\n",
        "val_dataset = TranslationDataset(val_de, val_en, de_tokenizer, en_tokenizer)\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=val_dataset.collate\n",
        ")\n",
        "\n",
        "\n",
        "def get_val_loss(model, batches=1):\n",
        "    counter = 1\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    for de_batch, en_batch in val_dataloader:\n",
        "        logits = model(de_batch, en_batch[:, :-1])\n",
        "        B, T, C = logits.shape\n",
        "        logits, en_batch = logits.reshape((B * T, C)), en_batch[:, 1:].reshape(-1)\n",
        "        loss = F.cross_entropy(logits, en_batch)\n",
        "        losses.append(loss.item())\n",
        "        if counter == batches:\n",
        "            break\n",
        "        counter += 1\n",
        "    model.train()\n",
        "    return sum(losses) / len(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihQAOPMEuMMC",
        "outputId": "4aff39a0-5729-4a95-96cf-6c87562569e3"
      },
      "outputs": [],
      "source": [
        "DE_VOCAB_SIZE = de_tokenizer.size\n",
        "EN_VOCAB_SIZE = en_tokenizer.size\n",
        "EMBEDDING_SIZE, MAX_LEN = 176, 500\n",
        "ENCODING_LAYERS, ENCODING_HEADS = 4, 4\n",
        "DECODING_LAYERS, DECODING_HEADS = 4, 4\n",
        "DE_PAD_ID, EN_PAD_ID = train_dataset.src_pad_id, train_dataset.tgt_pad_id\n",
        "\n",
        "model = Transformer.spawn(\n",
        "    in_vocab_size=DE_VOCAB_SIZE,\n",
        "    out_vocab_size=EN_VOCAB_SIZE,\n",
        "    embedding_size=EMBEDDING_SIZE,\n",
        "    max_len=MAX_LEN,\n",
        "    enc_layers=ENCODING_LAYERS,\n",
        "    dec_layers=DECODING_LAYERS,\n",
        "    enc_heads=ENCODING_HEADS,\n",
        "    dec_heads=DECODING_HEADS,\n",
        "    src_pad_id=DE_PAD_ID,\n",
        "    tgt_pad_id=EN_PAD_ID,\n",
        ")\n",
        "print(f\"{get_param_count(model) / 1e6} mn parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z39vbT1euMMC"
      },
      "outputs": [],
      "source": [
        "kaiming_init(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lSFA_DikuMMD"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sLP3utLVWANz"
      },
      "outputs": [],
      "source": [
        "mean_train_loss = None\n",
        "mean_val_loss = None\n",
        "print_interval = 20\n",
        "epochs = 100\n",
        "grads = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbWK01iOuMME",
        "outputId": "680d6129-8141-4a6c-bbee-e46b058e5efc"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    for counter, (de_batch, en_batch) in enumerate(train_dataloader):\n",
        "        logits = model(de_batch, en_batch[:, :-1])\n",
        "        B, T, C = logits.shape\n",
        "        logits, en_batch = logits.reshape((B * T, C)), en_batch[:, 1:].reshape(-1)\n",
        "        train_loss = F.cross_entropy(logits, en_batch)\n",
        "        mean_train_loss = (\n",
        "            mean_train_loss if mean_train_loss is not None else train_loss.item()\n",
        "        ) * 0.995 + train_loss.item() * 0.005\n",
        "        if counter % print_interval == 0:\n",
        "            val_loss = get_val_loss(model)\n",
        "            mean_val_loss = (\n",
        "                mean_val_loss if mean_val_loss is not None else val_loss\n",
        "            ) * 0.99 + val_loss * 0.01\n",
        "            print(\n",
        "                \"%d:%d -> %.4f(%.4f), %.4f(%.4f)\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    counter + 1,\n",
        "                    train_loss.item(),\n",
        "                    mean_train_loss,\n",
        "                    val_loss,\n",
        "                    mean_val_loss,\n",
        "                )\n",
        "            )\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        grads.append(\n",
        "            [\n",
        "                mod.weight.grad.abs().mean().item()\n",
        "                for _, mod in model.named_modules()\n",
        "                if hasattr(mod, \"weight\")\n",
        "            ]\n",
        "        )\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p48hkSa8M92v"
      },
      "outputs": [],
      "source": [
        "# Peer into gradients\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gradients = torch.tensor(grads)\n",
        "layers = torch.tensor([idx for idx in range(len(gradients[0]))])\n",
        "batches = torch.tensor([idx for idx in range(len(gradients))])\n",
        "\n",
        "plt.contourf(layers.numpy(), batches.numpy(), gradients.numpy(), cmap=\"viridis\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AlhGla6QVzo"
      },
      "outputs": [],
      "source": [
        "# Save model parameters\n",
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    \"emb_%d_enc_lays_%d_enc_heads_%d_dec_lays_%d_dec_heads_%d_train_loss_%.4f_val_loss_%.4f.pth\"\n",
        "    % (\n",
        "        EMBEDDING_SIZE,\n",
        "        ENCODING_LAYERS,\n",
        "        ENCODING_HEADS,\n",
        "        DECODING_LAYERS,\n",
        "        DECODING_HEADS,\n",
        "        mean_train_loss,\n",
        "        mean_val_loss,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp-N1xZ2uMMF"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "def predict(src_tokens, bos_id, eos_id):\n",
        "    input = torch.tensor([src_tokens], device=device)\n",
        "    context = torch.tensor([[bos_id]], device=device)\n",
        "    while True:\n",
        "        logits = model(input, context)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        probs = probs.view(-1, probs.shape[-1])\n",
        "        choices = torch.multinomial(probs, num_samples=1)\n",
        "        choices = choices[-1, :]\n",
        "        if choices.item() == eos_id:\n",
        "            break\n",
        "        context = torch.cat((context, choices.unsqueeze(0)), dim=1)\n",
        "    return context[0].tolist()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
