{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from core.tokenizer.regex import RegexTokenizer\n",
    "\n",
    "train_de = \".data/train.de\"\n",
    "train_en = \".data/train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_UNK, TOKEN_PAD, TOKEN_BOS, TOKEN_EOS = \"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"\n",
    "special_tokens = [TOKEN_BOS, TOKEN_EOS, TOKEN_PAD, TOKEN_UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(filepath, vocab_size, cache=None):\n",
    "    tokenizer = RegexTokenizer()\n",
    "    if cache is not None:\n",
    "        tokenizer.load(cache + \".model\")\n",
    "    else:\n",
    "        text = \"\".join(open(filepath, encoding=\"utf-8\").read().splitlines())\n",
    "        tokenizer.train(text, vocab_size)\n",
    "        tokenizer.register_special_tokens({token: tokenizer.size + idx for idx, token in enumerate(special_tokens)})\n",
    "    tokenizer.save(cache)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = get_tokenizer(train_de, 512, \"tokenizers/de\")\n",
    "en_tokenizer = get_tokenizer(train_en, 512, \"tokenizers/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, src_tokenizer, tgt_tokenizer):\n",
    "        super().__init__()\n",
    "        self.source = [src_tokenizer.encode(line) for line in open(src_file, encoding=\"utf-8\").read().splitlines()]\n",
    "        self.target = [tgt_tokenizer.encode(line) for line in open(tgt_file, encoding=\"utf-8\").read().splitlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.source[index], self.target[index])\n",
    "\n",
    "dataset = TranslationDataset(train_de, train_en, de_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    de_batch, en_batch = [], []\n",
    "    de_bos, de_eos = de_tokenizer.special_tokens[TOKEN_BOS], de_tokenizer.special_tokens[TOKEN_EOS]\n",
    "    en_bos, en_eos = en_tokenizer.special_tokens[TOKEN_BOS], en_tokenizer.special_tokens[TOKEN_EOS]\n",
    "    for de_item, en_item in batch:\n",
    "        de_batch.append(torch.cat((torch.tensor([de_bos]), torch.tensor(de_item), torch.tensor([de_eos]))))\n",
    "        en_batch.append(torch.cat((torch.tensor([en_bos]), torch.tensor(en_item), torch.tensor([en_eos]))))\n",
    "    de_batch = pad_sequence(de_batch, padding_value=de_tokenizer.special_tokens[TOKEN_PAD], batch_first=True)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=en_tokenizer.special_tokens[TOKEN_PAD], batch_first=True)\n",
    "    return de_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models import Transformer\n",
    "\n",
    "DE_VOCAB_SIZE = de_tokenizer.size\n",
    "EN_VOCAB_SIZE = en_tokenizer.size\n",
    "EMBEDDING_SIZE, MAX_LEN = 256, 200\n",
    "ENCODING_LAYERS, ENCODING_HEADS = 10, 4\n",
    "DECODING_LAYERS, DECODING_HEADS = 10, 4  \n",
    "DE_PAD_ID, EN_PAD_ID = de_tokenizer.special_tokens[TOKEN_PAD], en_tokenizer.special_tokens[TOKEN_PAD]\n",
    "\n",
    "model = Transformer(\n",
    "    in_vocab_size=DE_VOCAB_SIZE,\n",
    "    out_vocab_size=EN_VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    enc_layers=ENCODING_LAYERS,\n",
    "    dec_layers=DECODING_LAYERS,\n",
    "    enc_heads=ENCODING_HEADS,\n",
    "    dec_heads=DECODING_HEADS,\n",
    "    src_pad_id=DE_PAD_ID,\n",
    "    tgt_pad_id=EN_PAD_ID,\n",
    ")\n",
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000\n",
    "print(f\"{param_count} mn parameters\")\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        torch.nn.init.kaiming_uniform(m.weight.data)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss = None\n",
    "print_interval = 20\n",
    "epochs = 100\n",
    "\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        for counter, (de_batch, en_batch) in enumerate(train_dataloader):\n",
    "            logits = model(de_batch, en_batch[:, :-1])\n",
    "            B, T, C = logits.shape\n",
    "            logits, en_batch = logits.reshape((B*T, C)), en_batch[:,1:].reshape(-1)\n",
    "            loss = F.cross_entropy(logits, en_batch)\n",
    "            mean_loss = (mean_loss if mean_loss is not None else loss.item()) * 0.99 + loss.item() * 0.01\n",
    "            if counter % print_interval == 0:\n",
    "                print(f\"{epoch + 1} -> {counter + 1}: current loss: {loss.item()}, mean loss: {mean_loss}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    while True:\n",
    "        winsound.Beep(1000, 1000)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"embedding_{EMBEDDING_SIZE}_enc_layers_{ENCODING_LAYERS}_enc_heads_{ENCODING_HEADS}_dec_layers_{DECODING_LAYERS}_dec_heads_{DECODING_HEADS}_loss_{mean_loss}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"embedding_256_enc_layers_10_enc_heads_4_dec_layers_10_dec_heads_4_loss_3.068118011436385.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for de_batch, en_batch in train_dataloader:\n",
    "    break\n",
    "de_batch, en_batch = de_batch[0:1, :], en_batch[0:1, :]\n",
    "context = torch.tensor([[en_tokenizer.special_tokens[TOKEN_BOS]]])\n",
    "\n",
    "while True:\n",
    "    logits = model(de_batch, context)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs = probs.view(-1, probs.shape[-1])\n",
    "    choices = torch.multinomial(probs, num_samples=1)\n",
    "    choices = choices[-1, :]\n",
    "    if choices.item() == en_tokenizer.special_tokens[TOKEN_EOS]:\n",
    "        break\n",
    "    context = torch.cat((context, choices.unsqueeze(0)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_tokenizer.decode(context[0].tolist()))\n",
    "print(en_tokenizer.decode(en_batch[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
