{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/datasets/ocr_data.zip .\n",
    "!ls /content/drive/MyDrive/ocr_params/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /content/drive/MyDrive/ocr_params/ocr_* ocr_params.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"ocr_data.zip\"):\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        print(\"upload images\")\n",
    "elif platform.system() == \"Linux\":\n",
    "    os.system(\n",
    "        \"git clone https://github.com/n1teshy/seq-transduction && mv seq-transduction/core . && rm -rf seq-transduction\"\n",
    "    )\n",
    "    os.system(\n",
    "        \"git clone https://github.com/n1teshy/cache && mv cache/ocr/tokenizers . && rm -rf cache\"\n",
    "    )\n",
    "    os.system(\"unzip ocr_data.zip -d . > /dev/null && rm ocr_data.zip\")\n",
    "else:\n",
    "    os.system(\n",
    "        \"git clone https://github.com/n1teshy/seq-transduction & move seq-transduction/core . & rd /s /q seq-transduction\"\n",
    "    )\n",
    "    os.system(\n",
    "        \"git clone https://github.com/n1teshy/cache & move cache/ocr/tokenizers . & rd /s /q cache\"\n",
    "    )\n",
    "    os.system(\n",
    "        \"powershell Expand-Archive -Path ocr_data.zip -DestinationPath . > NUL & del ocr_data.zip\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import signal\n",
    "import threading\n",
    "from collections import deque\n",
    "from core.config import device\n",
    "import torch.nn.functional as F\n",
    "from core.models import OCR, resnet14 as cnn_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from core.datasets.image import OCRDataset\n",
    "from core.tokenizers.regex import get_tokenizer\n",
    "from core.utils import get_param_count, DualLogger, kaiming_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = DualLogger(\"model.log\")\n",
    "interruption = threading.Event()\n",
    "signal.signal(signal.SIGINT, lambda _, __: interruption.set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.003\n",
    "EMBEDDING_SIZE = 256\n",
    "VOCAB_SZE = None\n",
    "MAX_LEN = 100\n",
    "DEC_LAYERS = 5\n",
    "DEC_HEADS = 8\n",
    "PADDING_ID = None\n",
    "MIN_PROGRESS = 0.05\n",
    "MAX_LOSS_DIFF = 0.25\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_FOLDER = \"data/train\"\n",
    "VAL_FOLDER = \"data/test\"\n",
    "ENCODER = cnn_fn(num_classes=EMBEDDING_SIZE)\n",
    "\n",
    "mean_window = 400\n",
    "accumulation_steps = 4\n",
    "last_saved_at = float(\"inf\")\n",
    "cur_loss_wt = 1 / mean_window\n",
    "mn_loss_wt = 1 - cur_loss_wt\n",
    "t_loss_sum, v_loss_sum = 0, 0\n",
    "t_losses, v_losses = deque(maxlen=mean_window), deque(maxlen=mean_window)\n",
    "param_dir = f\"cnn_{cnn_fn.__name__}_emb_{EMBEDDING_SIZE}_lyrs_{DEC_LAYERS}_hds_{DEC_HEADS}_mxlen_{MAX_LEN}\"\n",
    "os.makedirs(param_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"\", 256, \"tokenizers/en\")\n",
    "VOCAB_SZE = tokenizer.size\n",
    "\n",
    "train_dataset = OCRDataset(\n",
    "    TRAIN_FOLDER, mapping_file=\"meta/labels.txt\", tokenizer=tokenizer\n",
    ")\n",
    "val_dataset = OCRDataset(\n",
    "    VAL_FOLDER, mapping_file=\"meta/labels.txt\", tokenizer=tokenizer\n",
    ")\n",
    "PADDING_ID = train_dataset.pad_id\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=train_dataset.collate, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, collate_fn=val_dataset.collate, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "logger.log(f\"train dataloader: {len(train_dataloader)} batches\")\n",
    "logger.log(f\"val dataloader: {len(val_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OCR.spawn(\n",
    "    encoder=ENCODER.to(device),\n",
    "    out_vocab_size=VOCAB_SZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    dec_layers=DEC_LAYERS,\n",
    "    dec_heads=DEC_HEADS,\n",
    "    tgt_pad_id=PADDING_ID,\n",
    ")\n",
    "# model.load_state_dict(torch.load(\"ocr_params.pth\", map_location=device))\n",
    "kaiming_init(model)\n",
    "model_param_count = get_param_count(model)\n",
    "resnet_param_count = get_param_count(ENCODER)\n",
    "logger.log(\n",
    "    \"parameters: %.4fmn (%.4f + %.4f)\"\n",
    "    % (\n",
    "        model_param_count / 1e6,\n",
    "        resnet_param_count / 1e6,\n",
    "        (model_param_count - resnet_param_count) / 1e6,\n",
    "    )\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_loss_record(split, loss):\n",
    "    global t_loss_sum, v_loss_sum\n",
    "    losses = t_losses if split == \"train\" else v_losses\n",
    "    first_val = 0\n",
    "    if len(losses) == losses.maxlen:\n",
    "        first_val = losses.popleft()\n",
    "    if split == \"train\":\n",
    "        t_loss_sum += (loss - first_val)\n",
    "    else:\n",
    "        v_loss_sum += (loss - first_val)\n",
    "    losses.append(loss)\n",
    "    return (t_loss_sum if split == \"train\" else v_loss_sum) / len(losses)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_loss(split, batches=accumulation_steps):\n",
    "    model.eval()\n",
    "    iters, acc_loss = 0, 0\n",
    "    dataloader = train_dataloader if split == \"train\" else val_dataloader\n",
    "    for pixels, tokens in dataloader:\n",
    "        logits = model(pixels, tokens[:, :-1])\n",
    "        B, T, C = logits.shape\n",
    "        logits, tokens = logits.reshape((B * T, C)), tokens[:, 1:].reshape(-1)\n",
    "        loss = F.cross_entropy(logits, tokens)\n",
    "        acc_loss += loss.item()\n",
    "        if iters == batches:\n",
    "            break\n",
    "        iters += 1\n",
    "    model.train()\n",
    "    return acc_loss / batches\n",
    "\n",
    "\n",
    "def get_losses():\n",
    "    return get_loss(\"train\"), get_loss(\"val\")\n",
    "\n",
    "\n",
    "def save_model(t_loss, v_loss, folder=param_dir, checkpoint=False):\n",
    "    filename = \"ocr_%.4f_%.4f_class_%d_lr_%.4f.%s\" % (\n",
    "        t_loss,\n",
    "        v_loss,\n",
    "        EMBEDDING_SIZE,\n",
    "        LEARNING_RATE,\n",
    "        \"chk\" if checkpoint else \"pth\",\n",
    "    )\n",
    "    torch.save(model.state_dict(), os.path.join(folder, f\"{filename}\"))\n",
    "    logger.log(\"saved model with losses %.4f/%.4f at %s\" % (t_loss, v_loss, filename))\n",
    "\n",
    "\n",
    "def progress_monitor(e_no, b_no, t_loss, v_loss, mt_loss, mv_loss):\n",
    "    global last_saved_at\n",
    "    logger.log(\n",
    "        \"%d:%d -> %.4f(mean:%.4f), %.4f(mean:%.4f)\"\n",
    "        % (e_no, b_no, t_loss, mt_loss, v_loss, mv_loss)\n",
    "    )\n",
    "    if e_no * b_no >= mean_window and last_saved_at - mv_loss >= MIN_PROGRESS:\n",
    "        if abs(mt_loss - mv_loss) < MAX_LOSS_DIFF:\n",
    "            save_model(mt_loss, mv_loss)\n",
    "            last_saved_at = mv_loss\n",
    "        else:\n",
    "            word = \"overfitting\" if mt_loss - mv_loss > 0 else \"underfitting\"\n",
    "            logger.log(f\"the model seems to be {word}\")\n",
    "    if interruption.is_set():\n",
    "        if input(\"save checkpoint? \").strip().startswith(\"y\"):\n",
    "            save_model(mt_loss, mv_loss, checkpoint=True)\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e_no in range(1, EPOCHS + 1):\n",
    "    for b_no, (pixels, tokens) in enumerate(train_dataloader, start=1):\n",
    "        try:\n",
    "            logits = model(pixels, tokens[:, :-1])\n",
    "            B, T, C = logits.shape\n",
    "            logits, tokens = logits.reshape((B * T, C)), tokens[:, 1:].reshape(-1)\n",
    "            t_loss = F.cross_entropy(logits, tokens)\n",
    "            (t_loss / accumulation_steps).backward()\n",
    "            if b_no % accumulation_steps == 0 or b_no == len(train_dataloader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                t_loss, v_loss = get_losses()\n",
    "                mt_loss = update_loss_record(\"train\", t_loss)\n",
    "                mv_loss = update_loss_record(\"val\", v_loss)\n",
    "                progress_monitor(e_no, b_no, t_loss, v_loss, mt_loss, mv_loss)\n",
    "        except Exception:\n",
    "            save_model(mt_loss, mv_loss, checkpoint=True)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, bos_id, eos_id):\n",
    "    context = torch.tensor([[bos_id]], device=device)\n",
    "    while True:\n",
    "        logits = model(image, context)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs = probs.view(-1, probs.shape[-1])\n",
    "        choices = torch.multinomial(probs, num_samples=1)\n",
    "        choices = choices[-1, :]\n",
    "        if choices.item() == eos_id:\n",
    "            break\n",
    "        context = torch.cat((context, choices.unsqueeze(0)), dim=1)\n",
    "    return context[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, tgt_tokens in val_dataloader:\n",
    "    image = images[0].unsqueeze(0)\n",
    "    pred_tokens = predict(image, val_dataset.bos_id, val_dataset.eos_id)\n",
    "    print(tokenizer.decode(tgt_tokens[0].tolist()), tokenizer.decode(pred_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
