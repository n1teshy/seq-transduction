{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer files\n",
    "!git clone https://github.com/n1teshy/cache && mv cache/tokenizers . && rm -rf cache\n",
    "# Extract data\n",
    "!unzip ocr_data.zip -d . > /dev/null && rm ocr_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from core.models import OCR, resnet18\n",
    "from core.datasets.image import OCRDataset\n",
    "from core.tokenizers.regex import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from core.utils import get_param_count\n",
    "from core.config import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER, BATCH_SIZE = \"data/textract_images/train\", 32\n",
    "TEST_FOLDER = \"data/textract_images/test\"\n",
    "tokenizer = get_tokenizer(\"_.txt\", 384, \"tokenizers/en\")\n",
    "train_dataset = OCRDataset(\n",
    "    TRAIN_FOLDER, mapping_file=\"meta/mapping.txt\", tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = OCRDataset(\n",
    "    TEST_FOLDER, mapping_file=\"meta/mapping.txt\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=train_dataset.collate, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=test_dataset.collate, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "def get_test_loss(model):\n",
    "    with torch.no_grad():\n",
    "        for pixels, tokens in test_dataloader:\n",
    "            logits = model(pixels, tokens[:, :-1])\n",
    "            B, T, C = logits.shape\n",
    "            logits, tokens = logits.reshape((B * T, C)), tokens[:, 1:].reshape(-1)\n",
    "            return F.cross_entropy(logits, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_SIZE = 256\n",
    "VOCAB_SZE = tokenizer.size\n",
    "MAX_LEN = 100\n",
    "DEC_LAYERS = 5\n",
    "DEC_HEADS = 4\n",
    "PADDING_ID = train_dataset.pad_id\n",
    "ENCODER = resnet18(num_classes=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OCR.spawn(\n",
    "    encoder= ENCODER.to(device),\n",
    "    out_vocab_size=VOCAB_SZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    dec_layers=DEC_LAYERS,\n",
    "    dec_heads=DEC_HEADS,\n",
    "    tgt_pad_id=PADDING_ID,\n",
    ")\n",
    "print(f\"{get_param_count(model)/1e6} mn params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_loss, mean_test_loss = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    for batch, (pixels, tokens) in enumerate(train_dataloader, start=1):\n",
    "        logits = model(pixels, tokens[:, :-1])\n",
    "        B, T, C = logits.shape\n",
    "        logits, en_batch = logits.reshape((B * T, C)), tokens[:, 1:].reshape(-1)\n",
    "        train_loss = F.cross_entropy(logits, en_batch)\n",
    "        mean_train_loss = (\n",
    "            mean_train_loss or train_loss.item()\n",
    "        ) * 0.99 + train_loss.item() * 0.01\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        test_loss = get_test_loss(model)\n",
    "        mean_test_loss = (\n",
    "            mean_test_loss or test_loss.item()\n",
    "        ) * 0.99 + test_loss.item() * 0.01\n",
    "        print(\n",
    "            \"%d:%d -> %.4f(mean:%.4f), %.4f(mean:%.4f)\"\n",
    "            % (\n",
    "                epoch,\n",
    "                batch,\n",
    "                train_loss.item(),\n",
    "                mean_train_loss,\n",
    "                test_loss.item(),\n",
    "                mean_test_loss,\n",
    "            )\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
